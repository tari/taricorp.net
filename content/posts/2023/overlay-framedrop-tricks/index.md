---
title: Improved timelapse video processing
slug: overlay-framedrop-tricks
date: 2023-08-30T05:58:37.811Z
---

Using the `ffmpeg`'s overlay filter is a handy trick.

Filter docs are pretty good: https://ffmpeg.org/ffmpeg-filters.html

---

The important thing is that the `overlay` filter attempts to align its inputs to the same *presentation timestamp* (PTS);
that is, the instant from the start of the video at which a given frame is to be displayed. Although this behavior is
not obvious, the documentation provides a few hints:

> It takes two inputs and has one output. The first input is the "main" video on which the second input is overlaid.
>
> Be aware that frames are taken from each input video in timestamp order, hence, if their initial timestamps differ,
> it is a good idea to pass the two inputs through a `setpts=PTS-STARTPTS` filter to have them begin in the same zero
> timestamp, as the example for the *movie* filter does.

This wording seems to imply that the PTS of each frame from the "main" input to the `overlay` filter is will be the PTS
of the output frame as well, and the other input will attempt to match that PTS from the other stream in order to select
an appropriate frame.

Taking that logic to the meaninful conclusion, if the "main" input to an `overlay` drops frames without modifying their
PTS and the other input to the filter has the same frame timing, then the overlaid image (from the second input) will
skip the same frames as the main input.

---

What this means is that `overlay` allows processing of frames to split, with one branch doing some filtering to
determine which frame should be kept and which should be retained, while the final output can be a copy of the input by
passing through the original input as the second input to the `overlay`. This may be easier to understand with a
diagrammed example:

{{% figure src="overlay-sample.drawio.svg"
    caption=`Flowchart illustrating a basic filter chain using the result of one filter to determine whether
             frames should be retained, but emitting the original frames.`
%}}

In this example we might imagine that the goal is to drop parts of the original video that are very dark, perhaps 
because we want to display it on something with poor contrast and converting to monochrome makes it easier to decide
whether a given video frame is too dark to be useful. Frames from the input video are split (copied) to:

1. A filter sequence that converts to monochrome then discards some of the frames based on the monochrome result,
   as if perhaps this video were meant to be displayed on a screen with poor contrast and we wanted to ignore parts
   of the video that would be too dark to see well.
2. Do nothing (no-op)

The first branch is the used as the main input to an overlay in order to set the desired output PTS, but because the
second branch (as a copy of the input) is fully opaque the result is to only select the input frames that were not
dropped by the filters on the first branch.

## A worked example

Recalling that the motivation for this filter layout was in processing time-lapse video, we can create a short
sample video and illustrate what happens when running various filters, arriving at a clear understanding of how the 
core understanding of careful `overlay` use is useful to generate good-looking time-lapse videos.

Here's an initial video, which is simply a counter that ticks off centiseconds and a rotating color bar as 
generated by the `testsrc` filter.[^demo-video-generation]

[^demo-video-generation]: for those playing along at home, I rendered this video with `ffmpeg -f lavfi -i 
    testsrc=duration=10:decimals=2 outfile.webm`. `-f lavfi` says the input format is a filter specification, and 
    `testsrc=duration=10` specifies a test video source which generates 10 seconds of video with the `decimals=2` 
    component making the displayed numbers show centiseconds rather than only whole seconds. I've opted to otherwise
    emit WebM video with default settings (VP9 codec at CRF 32).

<figure>
    <video controls autoplay loop playsinline width="320" height="240">
        <source src="testsrc-original.webm" type="video/webm">
    </video>
    <figcaption>Sample video with no processing applied.</figcaption>
</figure>

### Simple overlay

To become acquainted with `overlay`, we can add some decorations on top of the video: I've drawn
a crude smiley face to put on top of the test pattern. This image has a transparent background so it won't completely 
hide the video, and it has the same dimensions.

{{% figure src="crude-smiley.png" width=320 height=240
    caption="A crudely hand-drawn smiley face to overlay on the video." %}}

#### Complex filter specification

In order to actually use an overlay, it becomes necessary to know how to express a filter chain with multiple inputs.
The ffmpeg documentation provides fairly clear documentation, but for ease of reading I will summarize it here.
Thus far the filter chain has been strictly linear (one filter feeds immediately into the input of the next), which
can be written simply by separating the specification of each filter with a comma, like `filter1,filter2`.

For more complex filter chains, input and output connections can be given names, and unrelated filters are separated
by semicolons rather than commas. When connections of two filters have the same name, they are connected together: the
preceding example could also be written as `filter1 [out1]; [out1] filter2`, creating a "wire" or "pad" of sorts named 
`out1` that the output of `filter1` (because `[out1]` appears after the filter name) and the input of `filter2`
(`[out1]` written before `filter2`) are connected to.

---

To read two inputs from files, we also need to understand how to tell `ffmpeg` to read more than one file on the command
line, and how to refer to them with filters.
Input files are always specified with any number of options, then `-i infile` where `infile` is the name of the 
desired file. In this case (and in many others), no further options are required, so specifying multiple files can
be done simply by repeating `-i` options: `-i file1 -i file2`.

Each input file is assigned a number automatically, with the first input being assigned 0, then 1 for the next and so
forth. These numbers are the name of the corresponding pad in the filter chain, so a contrived filter chain consuming
two inputs and discarding the first (using a `nullsink` filter that has one input and no outputs) could be written
on the command line as `ffmpeg -i file1 -i file2 -filter_complex "[0] nullsink; [1] myfilter"` (where `myfilter` is
some arbitrarily-chosen filter). When a filter has multiple inputs (or outputs), multiple pad names may be 
specified in either position, like `[0][1] myfilter`.

Having established how to write these, all following samples will use the example video as input 0 and an image
for subsequent inputs, if used. A roughly equivalent `ffmpeg` command line is:

```
ffmpeg -i testsrc.webm -i image.png -filter_complex <filter_specification> out.webm
```

..where `testsrc.webm` is a file containing video, `image.png` is an image file and `<filter_specification>` 
describes the filter chain to use as described above.

#### Overlay in action

Returning to the sample video (input 0) and overlay image (input 1) and recalling that the first input is documented
to be the "main" input to an overlay (while the second input is placed on top of
it), it's easy to construct a filter chain that puts the image on top of our
video.

<figure>
    <video controls autoplay loop playsinline width="320" height="240">
        <source src="simple-overlay.webm" type="video/webm">
    </video>
    <figcaption><code>[0][1] overlay</code></figcaption>
</figure>

### Removing uninteresting frames

Pretending that we want to make a time-lapse from this test video, the `mpdecimate` filter is convenient. Its utility
for this application may not be immediately obvious from its documentation however:

> Drop frames that do not differ greatly from the previous frame in order to reduce frame rate.
>
> The main use of this filter is for very-low-bitrate encoding (e.g. streaming over dialup modem), but it could in 
> theory be used for fixing movies that were inverse-telecined incorrectly.

The first paragraph is the interesting one; we do in fact want to drop frames that do not differ greatly from their 
predecessors. It's probably similar to the use of `select` I
[previously suggested]({{% ref "/posts/2022/timelapser/index.md" %}})
for this application (`select=gt(scene\,0.02)`) that used a poorly-defined "scene metric" which was assumed to behave 
similarly. Overall, `mpdecimate` feels to me like a more appropriate choice.

<figure>
    <video controls autoplay loop playsinline width="320" height="240">
        <source src="testsrc-original.webm" type="video/webm">
    </video>
    <figcaption><code>[0] mpdecimate</code></figcaption>
</figure>

Since the sample video in use here has constant motion both in the color bar and counter, this example looks
exactly the same as its input.[^cheating-reuse]

[^cheating-reuse]: The actual video file embedded for this example is simply the
    original one, so you'll have to take my word that `mpdecimate` doesn't make
    any visual difference to the video here.

### Masking off motion

Since there's a lot of motion in the original video, decimation does nothing.
Supposing that we consider the sub-second portion (last two digits) of the
counter and the color bar to be uninteresting, they can be made to remain the
same by applying an overlay, in order to mask off the uninteresting parts of the
image.

I've made an overlay by taking the last frame of the video as a guide, then
manually drawing a box around the interesting part and making it transparent:

{{% figure src="testsrc-mask-seconds.png" width=320 height=240
    caption="The final frame of the video, with a box around the most significant (hundreds) digit of the counter removed and made transparent in order to mask out the rest of the image." %}}

I wasn't very careful to only capture the area the digit is displayed in, because
all that matters here is that the parts of the mask that are transparent only change
when I consider it interesting. Since the color bars that are passed through never
change, it's perfectly fine to skip masking them out.

Now if we apply this mask as an overlay on the original video, the last two digits
of the number will always be 96, since those are the values in the mask:

<figure>
    <video controls autoplay loop playsinline width="320" height="240">
        <source src="masked-96.webm" type="video/webm">
    </video>
    <figcaption><code>[0][1] overlay</code>, with the partially transparent mask image.</figcaption>
</figure>

There's a narrow dark line around the edge of the masked area, but otherwise this behaves
exactly as predicted; that line seems like an artifact of how the overlay works for some reason,
but it's not important because we'll be discarding the overlay result in the coming stages!

## Post-overlay decimation

Since there's suddenly a lot less motion in the video, removing uninteresting frames will
actually do something now:

<figure>
    <video controls autoplay loop playsinline width="320" height="240">
        <source src="overlay-decimate-96.webm" type="video/webm">
    </video>
    <figcaption><code>[0][1] overlay, mpdecimate</code></figcaption>
</figure>

This doesn't *look* any different from the first overlay, but it only has 10 frames rather
than the original's 250!
The difference is not visible because the PTS of each retained frame is unchanged,
so they get displayed at the same time as they were originally.

## Speeding up playback

To make it clear that we've removed a lot of frames, it would be nice to speed everything
up: the purpose of deleting those unchanging frames was to pretend the time they represent
didn't exist, so retaining the original PTS isn't very useful.

The `setpts` filter solves this problem, by allowing us to specify a new PTS for every frame
that passes through the filter, specified as a mathematical expression. Let's speed it up
to 10 fps, so the 10 frames that are output play back in only one second rather than 10.

<figure>
    <video controls autoplay loop playsinline width="320" height="240">
        <source src="overlay-decimate-setpts-96.webm" type="video/webm">
    </video>
    <figcaption><code>[0][1] overlay, mpdecimate, setpts=N/(10*TB), fps=10</code></figcaption>
</figure>

The expression `N/(10*TB)` specifies how the PTS of each frame should be computed. `N` is the
number of the frame being processed by the filter (0 for the first frame it receives, 1 for the
next and so on), and `TB` is the "timebase value" which is effectively the resolution of the
time for each PTS.

---

{{%math%}}
For a 25 FPS video, the timebase might be 1⁄25 of a second and the PTS of each frame in sequence
(assuming a constant rate of 25 fps) would be $0,1,2,\ldots$ The timebase could also be any number which
evenly divides 1⁄25 of a second, such as 1⁄50 in which case the PTSes would be $0,2,4,\ldots$ As long as
the desired time can be represented as an integer multiple of the timebase, it should be possible
to accurately specify the presentation time for each frame.

It's convenient for videos to work this
way, because some common framerates (such as 30 fps) make it impossible to precisely represent the
PTS of most frames in a finite number of decimal digits: they'd have to round, because the frame
PTSes of the first three frames are $0,0.0\overline{3},0.0\overline{6},\ldots$ In practice because
the PTS value of each frame has finite resolution, the actual values might be $0,0.0333,0.0667,\ldots$
which makes the second frame appear slightly earlier than intended and the third slightly later than intended.
As long as the timebase itself can be specified precisely (such as by specifying the numerator and denominator
of its time duration; 1⁄30 rather than $0.0\overline{3}$), then the PTS of each frame can be exactly represented.
{{%/math%}}

Knowing all this, the expression `1/(10*TB)` represents one tenth of a second: `TB` is some number that ffmpeg
uses to represent "there are *this many* timebase ticks per second". Multiplying by the number
of each frame that is encountered makes each frame advance from the previous by 1⁄10 of a second, giving us
a video at a constant 10 fps regardless of how many frames were dropped from the input or what their original
PTSes were.

TODO: I think the preceding paragraph is wrong.
https://stackoverflow.com/a/58268695/2658436

---

There's an extra `fps` filter at the end here which I haven't mentioned yet: since 

## stuff

Writing this as 

The result of this has the same duration as the original input video, but fewer frames because the PTS of each output
frame is retained. The `setpts` filter can fix this.

It also has a problem with being longer than intended, because `overlay` by default repeats the last frame in the
shorter of its inputs until both end: if any frames at the end of the input are dropped on the first branch, they'll
still be taken from the second. Fortunately the documentation includes a pointer to "framesync" options that are
relevant to some filters that accept multiple inputs, where we can discover that setting `shortest=1`
makes it stop when either stream ends, which is correct for this application.
