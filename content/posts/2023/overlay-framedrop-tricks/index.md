---
title: Improved timelapse video processing
slug: overlay-framedrop-tricks
date: 2023-08-30T05:58:37.811Z
---

Using the `ffmpeg`'s overlay filter is a handy trick.

Filter docs are pretty good: https://ffmpeg.org/ffmpeg-filters.html

---

The important thing is that the `overlay` filter attempts to align its inputs to the same *presentation timestamp* (PTS);
that is, the instant from the start of the video at which a given frame is to be displayed. Although this behavior is
not obvious, the documentation provides a few hints:

> It takes two inputs and has one output. The first input is the "main" video on which the second input is overlaid.
>
> Be aware that frames are taken from each input video in timestamp order, hence, if their initial timestamps differ,
> it is a good idea to pass the two inputs through a `setpts=PTS-STARTPTS` filter to have them begin in the same zero
> timestamp, as the example for the *movie* filter does.

This wording seems to imply that the PTS of each frame from the "main" input to the `overlay` filter is will be the PTS
of the output frame as well, and the other input will attempt to match that PTS from the other stream in order to select
an appropriate frame.

Taking that logic to the meaninful conclusion, if the "main" input to an `overlay` drops frames without modifying their
PTS and the other input to the filter has the same frame timing, then the overlaid image (from the second input) will
skip the same frames as the main input.

---

What this means is that `overlay` allows processing of frames to split, with one branch doing some filtering to
determine which frame should be kept and which should be retained, while the final output can be a copy of the input by
passing through the original input as the second input to the `overlay`. This may be easier to understand with a
diagrammed example:

{{% figure src="overlay-sample.drawio.svg"
    caption=`Flowchart illustrating a basic filter chain using the result of one filter to determine whether
             frames should be retained, but emitting the original frames.`
%}}

In this example we might imagine that the goal is to drop parts of the original video that are very dark, perhaps 
because we want to display it on something with poor contrast and converting to monochrome makes it easier to decide
whether a given video frame is too dark to be useful. Frames from the input video are split (copied) to:

1. A filter sequence that converts to monochrome then discards some of the frames based on the monochrome result,
   as if perhaps this video were meant to be displayed on a screen with poor contrast and we wanted to ignore parts
   of the video that would be too dark to see well.
2. Do nothing (no-op)

The first branch is the used as the main input to an overlay in order to set the desired output PTS, but because the
second branch (as a copy of the input) is fully opaque the result is to only select the input frames that were not
dropped by the filters on the first branch.

## A worked example

Recalling that the motivation for this filter layout was in processing time-lapse video, we can create a short
sample video and illustrate what happens when running various filters, arriving at a clear understanding of how the 
core understanding of careful `overlay` use is useful to generate good-looking time-lapse videos.

Here's an initial video, which is simply a counter that ticks off centiseconds and a rotating color bar as 
generated by the `testsrc` filter.[^demo-video-generation]

[^demo-video-generation]: for those playing along at home, I rendered this video with `ffmpeg -f lavfi -i 
    testsrc=duration=10:decimals=2 outfile.webm`. `-f lavfi` says the input format is a filter specification, and 
    `testsrc=duration=10` specifies a test video source which generates 10 seconds of video with the `decimals=2` 
    component making the displayed numbers show centiseconds rather than only whole seconds. I've opted to otherwise
    emit WebM video with default settings (VP9 codec at CRF 32).

<figure>
    <video controls autoplay loop playsinline width="320" height="240">
        <source src="testsrc-original.webm" type="video/webm">
    </video>
    <figcaption>Sample video with no processing applied.</figcaption>
</figure>

### Simple overlay

To become acquainted with `overlay`, we can add some decorations on top of the video: I've drawn
a crude smiley face to put on top of the test pattern. This image has a transparent background so it won't completely 
hide the video, and it has the same dimensions.

{{% figure src="crude-smiley.png" width=320 height=240
    caption="A crudely hand-drawn smiley face to overlay on the video." %}}

#### Complex filter specification

In order to actually use an overlay, it becomes necessary to know how to express a filter chain with multiple inputs.
The ffmpeg documentation provides fairly clear documentation, but for ease of reading I will summarize it here.
Thus far the filter chain has been strictly linear (one filter feeds immediately into the input of the next), which
can be written simply by separating the specification of each filter with a comma, like `filter1,filter2`.

For more complex filter chains, input and output connections can be given names, and unrelated filters are separated
by semicolons rather than commas. When connections of two filters have the same name, they are connected together: the
preceding example could also be written as `filter1 [out1]; [out1] filter2`, creating a "wire" or "pad" of sorts named 
`out1` that the output of `filter1` (because `[out1]` appears after the filter name) and the input of `filter2`
(`[out1]` written before `filter2`) are connected to.

---

To read two inputs from files, we also need to understand how to tell `ffmpeg` to read more than one file on the command
line, and how to refer to them with filters.
Input files are always specified with any number of options, then `-i infile` where `infile` is the name of the 
desired file. In this case (and in many others), no further options are required, so specifying multiple files can
be done simply by repeating `-i` options: `-i file1 -i file2`.

Each input file is assigned a number automatically, with the first input being assigned 0, then 1 for the next and so
forth. These numbers are the name of the corresponding pad in the filter chain, so a contrived filter chain consuming
two inputs and discarding the first (using a `nullsink` filter that has one input and no outputs) could be written
on the command line as `ffmpeg -i file1 -i file2 -filter_complex "[0] nullsink; [1] myfilter"` (where `myfilter` is
some arbitrarily-chosen filter). When a filter has multiple inputs (or outputs), multiple pad names may be 
specified in either position, like `[0][1] myfilter`.

Having established how to write these, all following samples will use the example video as input 0 and an image
for subsequent inputs, if used. A roughly equivalent `ffmpeg` command line is:

```
ffmpeg -i testsrc.webm -i image.png -filter_complex <filter_specification> out.webm
```

..where `testsrc.webm` is a file containing video, `image.png` is an image file and `<filter_specification>` 
describes the filter chain to use as described above.

#### Overlay in action

Returning to the sample video (input 0) and overlay image (input 1) and recalling that the first input is documented
to be the "main" input to an overlay (while the second input is placed on top of
it), it's easy to construct a filter chain that puts the image on top of our
video.

<figure>
    <video controls autoplay loop playsinline width="320" height="240">
        <source src="simple-overlay.webm" type="video/webm">
    </video>
    <figcaption><code>[0][1] overlay</code></figcaption>
</figure>

### Removing uninteresting frames

Pretending that we want to make a time-lapse from this test video, the `mpdecimate` filter is convenient. Its utility
for this application may not be immediately obvious from its documentation however:

> Drop frames that do not differ greatly from the previous frame in order to reduce frame rate.
>
> The main use of this filter is for very-low-bitrate encoding (e.g. streaming over dialup modem), but it could in 
> theory be used for fixing movies that were inverse-telecined incorrectly.

The first paragraph is the interesting one; we do in fact want to drop frames that do not differ greatly from their 
predecessors. It's probably similar to the use of `select` I
[previously suggested]({{% ref "/posts/2022/timelapser/index.md" %}})
for this application (`select=gt(scene\,0.02)`) that used a poorly-defined "scene metric" which was assumed to behave 
similarly. Overall, `mpdecimate` feels to me like a more appropriate choice.

<figure>
    <video controls autoplay loop playsinline width="320" height="240">
        <source src="testsrc-original.webm" type="video/webm">
    </video>
    <figcaption><code>[0] mpdecimate</code></figcaption>
</figure>

Since the sample video in use here has constant motion both in the color bar and counter, this example looks
exactly the same as its input.[^cheating-reuse]

[^cheating-reuse]: The actual video file embedded for this example is simply the
    original one, so you'll have to take my word that `mpdecimate` doesn't make
    any visual difference to the video here.

### Masking off motion

Since there's a lot of motion in the original video, decimation does nothing.
Supposing that we consider the sub-second portion (last two digits) of the
counter and the color bar to be uninteresting, they can be made to remain the
same by applying an overlay, in order to mask off the uninteresting parts of the
image.

I've made an overlay by taking the last frame of the video as a guide, then
manually drawing a box around the interesting part and making it transparent:

{{% figure src="testsrc-mask-seconds.png" width=320 height=240
    caption="The final frame of the video, with a box around the most significant (hundreds) digit of the counter removed and made transparent in order to mask out the rest of the image." %}}

I wasn't very careful to only capture the area the digit is displayed in, because
all that matters here is that the parts of the mask that are transparent only change
when I consider it interesting. Since the color bars that are passed through never
change, it's perfectly fine to skip masking them out.

Now if we apply this mask as an overlay on the original video, the last two digits
of the number will always be 96, since those are the values in the mask:

<figure>
    <video controls autoplay loop playsinline width="320" height="240">
        <source src="masked-96.webm" type="video/webm">
    </video>
    <figcaption><code>[0][1] overlay</code>, with the partially transparent mask image.</figcaption>
</figure>

There's a narrow dark line around the edge of the masked area, but otherwise this behaves
exactly as predicted; that line seems like an artifact of how the overlay works for some reason,
but it's not important because we'll be discarding the overlay result in the coming stages!

## Post-overlay decimation

Since there's suddenly a lot less motion in the video, removing uninteresting frames will
actually do something now:

<figure>
    <video controls autoplay loop playsinline width="320" height="240">
        <source src="overlay-decimate-96.webm" type="video/webm">
    </video>
    <figcaption><code>[0][1] overlay, mpdecimate</code></figcaption>
</figure>

This doesn't *look* any different from the first overlay, but it only has 10 frames rather
than the original's 250!
The difference is not visible because the PTS of each retained frame is unchanged,
so they get displayed at the same time as they were originally.

## Speeding up playback

To make it clear that we've removed a lot of frames, it would be nice to speed everything
up: the purpose of deleting those unchanging frames was to pretend the time they represent
didn't exist, so retaining the original PTS isn't very useful.

The `setpts` filter solves this problem, by allowing us to specify a new PTS for every frame
that passes through the filter, specified as a mathematical expression. Let's speed it up
to 10 fps, so the 10 frames that are output play back in only one second rather than 10.

<figure>
    <video controls autoplay loop playsinline width="320" height="240">
        <source src="overlay-decimate-setpts-96.webm" type="video/webm">
    </video>
    <figcaption><code>[0][1]&nbsp;overlay, mpdecimate, fps=10, setpts=N*TB</code></figcaption>
</figure>

The expression `N*TB` specifies how the PTS of each frame should be computed. `N` is the
number of the frame being processed by the filter (0 for the first frame it receives, 1 for the
next and so on), and `TB` is the "timebase value" which is effectively the resolution of the
time for each PTS.

---

{{%math%}}
For a 25 FPS video, the timebase might be 1⁄25 of a second and the PTS of each frame in sequence
(assuming a constant rate of 25 fps) would be $0,1,2,\ldots$ The timebase could also be any number which
evenly divides 1⁄25 of a second, such as 1⁄50 in which case the PTSes would be $0,2,4,\ldots$ As long as
the desired time can be represented as an integer multiple of the timebase, it should be possible
to accurately specify the presentation time for each frame.

It's convenient for videos to work this
way, because some common framerates (such as 30 fps) make it impossible to precisely represent the
PTS of most frames in a finite number of decimal digits: they'd have to round, because the frame
PTSes of the first three frames are $0,0.0\overline{3},0.0\overline{6},\ldots$ In practice because
the PTS value of each frame has finite resolution, the actual values might be $0,0.0333,0.0667,\ldots$
which makes the second frame appear slightly earlier than intended and the third slightly later than intended.
As long as the timebase itself can be specified precisely (such as by specifying the numerator and denominator
of its time duration; 1⁄30 rather than $0.0\overline{3}$), then the PTS of each frame can be exactly represented.
{{%/math%}}

---

Knowing all this, the expression `N*TB` following `fps=10` represents one tenth of a second:
`TB` is 1⁄10 of a second after `fps` sets it (otherwise the timebase is inherited from the input;
it would be 1⁄25 if we didn't set it manually in this case)
and multiplying by `N` makes the PTS advance monotonically by `TB` for each frame.

## Recovering masked parts

Having masked out high-motion parts of the frame to decimate, we lost all of the motion in the color
bars and were stuck seeing the mask itself. Now it's time to use the core `overlay` realization
that motivated this entire thing to recover the entire image for the frames that we want
to keep.

<figure>
    <video controls autoplay loop playsinline width="320" height="240">
        <source src="overlay-decimate-recover.webm" type="video/webm">
    </video>
    <figcaption><code>[0]&nbsp;split&nbsp;[x][y];
                      [x][1]&nbsp;overlay, mpdecimate&nbsp;[d];
                      [d][y]&nbsp;overlay</code></figcaption>
</figure>

Using `split` to make two copies of the input video and overlaying one copy on top of the
decimated stream allows us to see the entire input frame, as intended: the color bar rotates
and the centisecond counter is `00`, consistent with showing the first frame of each second
rather than the last like the mask image used.

However, not all is well: once the seconds digit reaches 9 (the last value we expect to see),
we see every frame until the end of the video! This is because `overlay` by default will
continue to emit data as long as any frames are available from either of its input: even though
the decimated stream has ended, the input hasn't yet so it repeats the last frame of the decimated
input until the overlay ends. Fortunately, one of the "framesync" options that `overlay`
accepts solves this problem: `shortest=1` will make it stop when either input ends.

<figure>
    <video controls autoplay loop playsinline width="320" height="240">
        <source src="overlay-decimate-shortest.webm" type="video/webm">
    </video>
    <figcaption><code>[0]&nbsp;split&nbsp;[x][y];
                      [x][1]&nbsp;overlay, mpdecimate&nbsp;[d];
                      [d][y]&nbsp;overlay=shortest=1</code></figcaption>
</figure>

## Putting it all together

Finally, we can demonstrate the entire pipeline with decimation and PTS recomputation
to output something that can truly be called a timelapse video.

<figure>
    <video controls autoplay loop playsinline width="320" height="240">
        <source src="final.webm" type="video/webm">
    </video>
    <figcaption><code>[0]&nbsp;split&nbsp;[x][y];
                      [x][1]&nbsp;overlay, mpdecimate&nbsp;[d];
                      [d][y]&nbsp;overlay=shortest=1,
                      fps=10, setpts=N*TB, photosensitivity</code></figcaption>
</figure>

As an extra trick that doesn't seem to matter for this example but is useful for actual
videos, I've added the `photosensitivity` filter which filters rapid changes in brightness
to be less rapid; I find the result is generally better-looking after that filter, and 
it should be safer for people who are vulnerable to photosensitive epilepsy or flicker
vertigo to view the result than it would be otherwise, which is good!

## Conclusions

I had a fun time gaining a better understanding of ffmpeg filters to achieve my goal
of making better time-lapse videos, and I hope the detailed writeup is useful to my
readers, either to gain a better understanding of video processing in general
or as a guide for generating your own time-lapse videos!
